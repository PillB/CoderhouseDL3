{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbOWs48ECBLUPrm1upEdou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PillB/CoderhouseDL3/blob/main/ProjectoFinal%2BIllescas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXqXUVWvwlav"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías necesarias\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "import requests\n",
        "\n",
        "# Descargar recursos necesarios de nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Descargar el texto de \"Pride and Prejudice\" desde Project Gutenberg\n",
        "# URL del texto completo en formato .txt\n",
        "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'\n",
        "response = requests.get(url)\n",
        "text_data = response.text\n",
        "\n",
        "# Convertir el texto en una lista de líneas para formar un dataframe\n",
        "lines = text_data.split('\\n')\n",
        "df = pd.DataFrame(lines, columns=['text'])\n",
        "\n",
        "# Mostrar las primeras filas del dataset para validar la carga\n",
        "print(\"Primeras filas del dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# 1. Tokenización\n",
        "# Explicación: La tokenización separa el texto en palabras individuales o 'tokens'.\n",
        "df['tokens'] = df['text'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# 2. Eliminación de Stopwords\n",
        "# Explicación: Las stopwords son palabras comunes que no aportan mucho valor analítico (e.g., 'el', 'la', 'y').\n",
        "stop_words = set(stopwords.words('english'))  # Cambiar a inglés porque el texto es en inglés\n",
        "df['tokens_sin_stopwords'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# 3. Lematización\n",
        "# Explicación: La lematización reduce las palabras a su forma base o lema.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmas'] = df['tokens_sin_stopwords'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# 4. Análisis de Sentimientos\n",
        "# Explicación: Analizamos la polaridad de cada oración usando TextBlob.\n",
        "df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "# 5. Vectorización TF-IDF\n",
        "# Explicación: El TF-IDF evalúa la importancia de palabras en el texto basado en frecuencia.\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Mostrar algunos resultados del preprocesamiento\n",
        "print(\"Resultados del preprocesamiento:\")\n",
        "print(df.head())\n",
        "\n",
        "# Verificar que se cumplieron todos los requerimientos\n",
        "# - Tokenización: Sí\n",
        "# - Eliminación de Stopwords: Sí\n",
        "# - Lematización: Sí\n",
        "# - Análisis de Sentimientos: Sí\n",
        "# - TF-IDF Vectorización: Sí\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias para el modelado de redes neuronales\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Cargar y preparar el dataset MNIST\n",
        "# Este conjunto de datos contiene imágenes de dígitos escritos a mano de 0 a 9\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalizar los valores de píxeles entre 0 y 1\n",
        "\n",
        "# Mostrar las dimensiones de los conjuntos de datos\n",
        "print(f\"Dimensiones del conjunto de entrenamiento: {x_train.shape}\")\n",
        "print(f\"Dimensiones del conjunto de prueba: {x_test.shape}\")\n",
        "\n",
        "# Mostrar la distribución de etiquetas en el conjunto de entrenamiento\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(f\"Distribución de etiquetas en el conjunto de entrenamiento: {dict(zip(unique, counts))}\")\n",
        "\n",
        "# Visualización exploratoria de algunas imágenes\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(6):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.imshow(x_train[i], cmap='gray')\n",
        "    plt.title(f\"Etiqueta: {y_train[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Construir y compilar un modelo de red neuronal simple con dos capas\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),        # Aplanar las imágenes de 28x28 píxeles en vectores 1D\n",
        "    Dense(128, activation='relu'),        # Primera capa densa con 128 neuronas y activación ReLU\n",
        "    Dense(10, activation='softmax')       # Capa de salida con 10 neuronas para la clasificación (0-9)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo en los datos de entrenamiento\n",
        "history = model.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Evaluar el modelo en los datos de prueba\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Precisión en los datos de prueba: {test_acc:.4f}\")\n",
        "\n",
        "# Graficar la precisión del entrenamiento y la validación\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'], label='Precisión del Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Precisión de la Validación')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Precisión')\n",
        "plt.title('Precisión de Entrenamiento y Validación')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Análisis Detallado de Resultados:\n",
        "# - Las precisiones de entrenamiento y validación muestran el proceso de aprendizaje.\n",
        "# - La precisión en el conjunto de prueba proporciona una medida de la capacidad de generalización del modelo.\n",
        "\n",
        "# Conclusiones\n",
        "# - El modelo actual proporciona una línea base con precisión aceptable.\n",
        "# - Las mejoras futuras pueden incluir agregar capas convolucionales o ajustar hiperparámetros.\n"
      ],
      "metadata": {
        "id": "dn-95YFkzitc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}